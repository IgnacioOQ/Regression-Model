---
title: "Regression Models Extended Project"
author: "Ignacio Ojea"
date: "July 28, 2018"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(car)
library(ggplot2)
library(magrittr)

```
# Executive Summary

This report is a course project within the Regression Models course on the Data Science Specialization by Johns Hopkins University on Coursera. I will examine the mtcars data set and explore how miles per gallon (MPG) is affected by different variables. In particularly, we will answer the following two questions: (1) Is an automatic or manual transmission better for MPG, and (2) Quantify the MPG difference between automatic and manual transmissions.

From the analysis I can show that manual transmission has an mpg **1.8** greater than an automatic transmission. Nevertheless, results show that it is **not** significant.

# Exploratory data analisis 
```{r }
data("mtcars")
?mtcars 
# am variable accounts for transmission, and mpg for miles per gallon. 
# am, cyl, vs, gear and carb are factors
mtcars2 <- mtcars #para la correlation table later
mtcars$am <- factor(mtcars$am,labels=c('Automatic','Manual'))
cols <- c("cyl", "vs", "gear", "carb")
mtcars[cols] <- lapply(mtcars[cols], factor)
fit <- lm(mpg ~am, mtcars)
summary(fit)$coefficients
summary(fit)$r.squared
```
See appendix for exploratory analysis plot.
From the plot, as well as the first exploratory linear regression **fit**, the answer to the first question seems positive (coefficient is positive, and p-value indicates significance). But we need to include other variables in order to avoid bias, since the R squared value is 0.36 thus telling us this model only explains us 36% of the variance.

# Regression Models

## Strategy for model selection
To begin, the type of data does not suggest the need to use either binomial or Poisson generalized linear models, since no variables correspond to binary (or sets of binary) outcomes, nor to counting, number of occurences, or rates. 

Hence we need to do a regular multivariable regression. The question now is which are the relevant predictor variables to take into account. Too few will lead to bias, too many to an increase in variance. We need a strategy for model selection. 
We start with an initial model **fitall** that takes into account all variables, and perfom stepwise model selection to select significant predictors for the final model which is the best model **fitbest** using the *step* function in R, which builds several regression models and makes a selection using the AIC algorithm. I will hide the results for the sake of simplicity, the code is below.

```{r, echo = 'true', results='hide'}
fitall <- lm(mpg ~ ., data = mtcars)
fitbest <- step(fitall, direction = "both")
```

Since the assignment needs to be short, I will avoid studying variance inflation factors (via the _vif_ function). Also, the _step_ function implicitly exhausts a nested model strategy. Rather, I will _anova_ to compare the three models we explored:
```{r }
anova(fit,fitbest,fitall)
```
The p-value for **fitbest** is significant, therefore we reject the null hypothesis that the added variables cyl, hp and wp are unnecessary. On the contrary, we see that **fitall** has a p-value larger than 0.05, which suggests that the added predictors may not be necessary.

Since the **fitbest** model is accurate, we want to explore how much of the variance it explicates. We do this computing R squared.

```{r }
summary(fitbest)$r.squared
summary(fitbest)$adj.r.squared
```
We therefore see that the linear relationship explains about 86% percent of the variance.

## Another strategy for model selection

Let’s have a closer look to the corelations of ‘mpg’ to the other variables of “mtcars”:

```{r }
res <- cor(mtcars2)
round(res, 2)
```
According to the correlation table, there are at least four variables with a high correlation to our outcome variable “mpg”. The highest value comes from the weight variable “wt”. Pero ojo porque aca algunas variables estan consideradas como numericas y no como factors. Pero con esta estrategia un poco podes ver cuales son las variables mas relevantes.

## Another strategy

We performed ANOVA (analysis of variance) to identify superfluous variables. The analysis pointed to cylinders, displacement, and weight as the only significant terms. Estos son las Pr(>F) de todas las variables.

```{r }
anova(fitall) %>% .[5] %>% t
```

# Inference

We also perform a t-test and we see that the manual and automatic transmissions are significatively different (by looking at p-value and confidence interval).

```{r }
Automatic <- mtcars[mtcars$am == "Automatic",]
Manual <- mtcars[mtcars$am == "Manual",]
t.test(Automatic$mpg, Manual$mpg)
```

# Residual diagnostics

See the appendix for Residual Plot.

```{r }
cov(fitbest$residuals, hatvalues(fitbest))
```

The points in the Residuals vs. Fitted plot, as well as the analysis of covariance suggest that the variables are independent, namely that there is not correlation between them, as desired. From the rest of the plots we also see that residuales all normally distributed and homoskedastic.

We also see that there are some outliers. I leave the  study leverage (by looking at *hatvalue(fitbest)*) and influence (by looking at *dfbetas(fitbest)*) too avoid making the project too long.

## More residual diagnostics

```{r }
#One idea
leverage <- hatvalues(fitbest)
tail(sort(leverage),3)
influential <- dfbetas(fitbest)
tail(sort(influential[,6]),3)

#Other idea
#Consistently with the residual plots, those points are the main responsibles for the 
#deviation from the residual normality assumption which is fullfilled within a confidence 
#interval of 0.05 as it can be seen below
influence <- sort(dffits(fitbest),decreasing=TRUE)
shapiro.test(fitbest$residuals)
```
For this reason (the shapiro test is not significant), those higher-influence points do not pose particular problems as they do not invalidate our conclusions about the final model.

# Conclusions

```{r }
summary(fitbest)
```
Based on the observations from our **fitbest** model, we can conclude the following,

* Cars with Manual transmission get more miles per gallon compared aganist cars with Automatic transmission. (1.8 adjusted by hp, cyl, and wt). mpg will decrease by 2.5 (adjusted by hp, cyl, and am) for every 1000 lb increase in wt.

* Nevertheless, the p-value for the transmission variable (amManual) is 0.2, which is widely above the 0.05 to consider it significant. Further exploration of the irrelevance of the transmisssion variable below.

* If number of cylinders, cyl increases from 4 to 6 and 8, mpg will decrease by a factor of 3 and 2.2 respectively (adjusted by hp, wt, and am).

# A case for the irrelevance of traansmission
To confirm and illustrate the limited impact that transmission has on mpg, we can make two regression models, one modeling mpg with cylinder number, weight, displacement, and  transmission type, and one without transmission, and then graph them below. The rear axle ratio is excluded because, despite being accepted earlier in the analysis, it has a p-value well above the 5% threshold needed to be genuinely considered relevant.
```{r }
fitnoam <- lm(mpg~ cyl + hp + wt,mtcars)
par(mfrow=c(1,2))
plot(fitnoam,which=1,main="without transmission")
plot(fitbest,which=1,main="with transmission")
summary(fitbest)$adj.r.squared
summary(fitnoam)$adj.r.squared
```
On inspection, these models are virtually identical, so we can confidently say that transmission type has no significant impact on a car’s mileage.

# Appendix with plots

## Exploratory analysis plot:
```{r, fig.width=3, fig.height=2}
g <- ggplot(mtcars) + aes(x = factor(am), y = mpg) + geom_boxplot() 
g <- g + stat_summary(fun.y=mean, geom="line", aes(group=1)) + stat_summary(fun.y=mean, geom="point") + ggtitle("MPG vs Transmission") + xlab("Transmission") + ylab("MPG")
g
```

## Residual Plots

Para plotear algunos y no todos los plots
```{r }
plot(fitbest,which=1)
```


```{r }
par(mfrow = c(2, 2))
plot(fitbest)
```