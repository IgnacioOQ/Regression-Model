---
title: "Regression Models Course Project"
author: "Ignacio Ojea"
date: "July 28, 2018"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(car)
library(ggplot2)
```
## Assignment
You work for Motor Trend, a magazine about the automobile industry. Looking at a data set of a collection of cars, they are interested in exploring the relationship between a set of variables and miles per gallon (MPG) (outcome). They are particularly interested in the following two questions:

1. “Is an automatic or manual transmission better for MPG”
2. "Quantify the MPG difference between automatic and manual transmissions"

## Exploratory data analisis 
```{r }
data("mtcars")
?mtcars 
# am variable accounts for transmission, and mpg for miles per gallon. 
# am, cyl, vs, gear and carb are factors
mtcars$am <- factor(mtcars$am,labels=c('Automatic','Manual'))
cols <- c("cyl", "vs", "gear", "carb")
mtcars[cols] <- lapply(mtcars[cols], factor)
fit <- lm(mpg ~am, mtcars)
summary(fit)$coefficients
summary(fit)$r.squared
```
See appendix for exploratory analysis plot.

From the plot, as well as the first exploratory linear regression **fit**, the answer to the first question seems positive (coefficient is positive, and p-value indicates significance). But we need to include other variables in order to avoid bias, since the R squared value is 0.36 thus telling us this model only explains us 36% of the variance.

# Regression Models

## Strategy for model selection
To begin, the type of data does not suggest the need to use either binomial or Poisson generalized linear models, since no variables correspond to binary (or sets of binary) outcomes, nor to counting, number of occurences, or rates. 

Hence we need to do a regular multivariable regression. The question now is which are the relevant predictor variables to take into account. Too few will lead to bias, too many to an increase in variance. We need a strategy for model selection. 
We start with an initial model **fitall** that takes into account all variables, and perfom stepwise model selection to select significant predictors for the final model which is the best model **fitbest** using the *step* function in R, which builds several regression models and makes a selection using the AIC algorithm. I will hide the results for the sake of simplicity, the code is below.

```{r, echo = 'true', results='hide'}
fitall <- lm(mpg ~ ., data = mtcars)
fitbest <- step(fitall, direction = "both")
```

Since the assignment needs to be short, I will avoid studying variance inflation factors (via the _vif_ function). Also, the _step_ function implicitly exhausts a nested model strategy. Rather, I will _anova_ to compare the three models we explored:
```{r }
anova(fit,fitbest,fitall)
```
The p-value for **fitbest** is significant, therefore we reject the null hypothesis that the added variables cyl, hp and wp are unnecessary. On the contrary, we see that **fitall** has a p-value larger than 0.05, which suggests that the added predictors may not be necessary.

Since the **fitbest** model is accurate, we want to explore how much of the variance it explicates. We do this computing R squared.

```{r }
summary(fitbest)$r.squared
```
We therefore see that the linear relationship explains about 86% percent of the variance, a great improvement from the 36% corresponding to **fit**.

## Inference

We also perform a t-test assuming that the transmission data has a normal distribution and we clearly see that the manual and automatic transmissions are significatively different (by looking at p-value and confidence interval).

```{r }
t.test(mpg ~ am, data = mtcars)
```

# Residual diagnostics

See the appendix for Residual Plot.

```{r }
cov(fitbest$residuals, hatvalues(fitbest))
```

The points in the Residuals vs. Fitted plot, as well as the analysis of covariance suggest that the variables are independent, namely that there is not correlation between them, as desired. From the rest of the plots we also see that residuales all normally distributed and homoskedastic.

We also see that there are some outliers. Let us study leverage and influence.
```{r }
leverage <- hatvalues(fitbest)
tail(sort(leverage),3)

influential <- dfbetas(fitbest)
tail(sort(influential[,6]),3)
```

# Answers to the questions
```{r }
summary(fitbest)
```
Based on the observations from our **fitbest** model, we can conclude the following,

* Cars with Manual transmission get more miles per gallon compared aganist cars with Automatic transmission. (1.8 adjusted by hp, cyl, and wt). mpg will decrease by 2.5 (adjusted by hp, cyl, and am) for every 1000 lb increase in wt.

* mpg decreases negligibly with increase of hp.

* If number of cylinders, cyl increases from 4 to 6 and 8, mpg will decrease by a factor of 3 and 2.2 respectively (adjusted by hp, wt, and am).

# Appendix with plots

## Exploratory analysis plot:
```{r, fig.width=4, fig.height=3}
g <- ggplot(mtcars) + aes(x = factor(am), y = mpg) + geom_boxplot() 
g <- g + stat_summary(fun.y=mean, geom="line", aes(group=1)) + stat_summary(fun.y=mean, geom="point") + ggtitle("MPG vs Transmission") + xlab("Transmission") + ylab("MPG")
g
```

## Residual Plots

```{r }
par(mfrow = c(2, 2))
plot(fitbest)
```